<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>E2E3D — Poster Schedule</title>

  <!-- reuse your existing assets/paths -->
  <link rel="stylesheet" href="./data/bulma.min.css">
  <link rel="stylesheet" href="./data/index.css">
  <link rel="stylesheet" href="./data/style.css">
  <link rel="icon" href="https://opensun3d.github.io/static/images/favicon.svg">
  <style>
    .subtitle time { font-weight: 600; }
    .board-col { white-space: nowrap; }
  </style>
</head>
<body>
  <section class="section">
    <div class="container is-max-desktop content">
      <h1 class="title">Poster Schedule</h1>
      <p class="subtitle">
        Poster Session: <time datetime="2025-10-19T14:35">Oct 19</time>, <time>14:35–15:40</time>
      </p>

      <table class="table is-fullwidth is-striped is-hoverable">
        <thead>
          <tr>
            <th class="board-col">Board</th>
            <th>#</th>
            <th>Title</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>118</td><td>1</td><td>PointSeg: A Training-Free Paradigm for 3D Scene Segmentation via Foundation Models</td></tr>
          <tr><td>119</td><td>5</td><td>Depth Anywhere: Enhancing 360 Monocular Depth Estimation via Perspective Distillation and Unlabeled Data Augmentation</td></tr>
          <tr><td>120</td><td>6</td><td>Constructing a 3D Town from a Single Image</td></tr>
          <tr><td>121</td><td>7</td><td>Semantic-Enhanced Monocular Depth Estimation via Fusion and Distillation of Foundation Models</td></tr>
          <tr><td>122</td><td>8</td><td>Beyond the Frame: Generating 360° Panoramic Videos from Perspective Videos</td></tr>
          <tr><td>123</td><td>9</td><td>STRIDE-QA: Visual Question Answering Dataset for Spatiotemporal Reasoning in Urban Driving Scenes</td></tr>
          <tr><td>124</td><td>12</td><td>PaintScene4D: Consistent 4D Scene Generation from Text Prompts</td></tr>
          <tr><td>125</td><td>14</td><td>Differentiable Room Acoustic Rendering with Multi-View Vision Priors</td></tr>
          <tr><td>126</td><td>15</td><td>Total-Editing: Head Avatar with Editable Appearance, Motion, and Lighting</td></tr>
          <tr><td>127</td><td>16</td><td>DAPS-AGF: Depth-Aware Perceptual Similarity with Adaptive Gradient Filtering for Enhanced Outdoor Scene Reconstruction</td></tr>
          <tr><td>178</td><td>17</td><td>GeoCD: A Differential Local Approximation for Geodesic Chamfer Distance</td></tr>
          <tr><td>179</td><td>18</td><td>Artist-Created Mesh Generation from Raw Observation</td></tr>
          <tr><td>180</td><td>19</td><td>S2GO: Streaming Sparse Gaussian Occupancy Prediction</td></tr>
          <tr><td>181</td><td>21</td><td>CIPHER: Culvert Inspection through Pairwise Frame Selection and High-Efficiency Reconstruction</td></tr>
          <tr><td>182</td><td>22</td><td>UNet-Based Keypoint Regression for Real-Time Cone Localization in Autonomous Racing</td></tr>
          <tr><td>183</td><td>23</td><td>Understanding multi-view transformers</td></tr>
          <tr><td>184</td><td>24</td><td>PU-Gaussian: Point Cloud Upsampling using 3D Gaussian Representation</td></tr>
          <tr><td>185</td><td>25</td><td>EVA-Gaussian: 3D Gaussian-based Real-time Human Novel View Synthesis under Diverse Multi-view Camera Settings</td></tr>
          <tr><td>186</td><td>26</td><td>DeepCollide: Scalable Data-Driven High DoF Configuration Space Modeling using Implicit Neural Representations</td></tr>
          <tr><td>187</td><td>27</td><td>CSG-Fusion: Consistent Sparse-View Gaussian Splatting via Matching-based Fusion</td></tr>
        </tbody>
      </table>

      <p><a class="button" href="../../iccv2025/index.html">← Back to workshop homepage</a></p>
    </div>
  </section>
</body>
</html>
